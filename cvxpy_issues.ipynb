{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# assigns weights to data points, currently all 1's just to test other functions\n",
    "def w(x, y, model):\n",
    "    return torch.ones(len(y))\n",
    "\n",
    "# samples minibatch\n",
    "def get_batch(x, y, batch_size):\n",
    "    inds = np.random.choice(range(len(y)), size=batch_size, replace=False)\n",
    "    return x[inds], y[inds]\n",
    "    \n",
    "# performs an iteration of the iteratively reweighted least squares algorithm\n",
    "def step(x, y, model, optimizer, w, batch_size):\n",
    "    # arguments are self explanatory except w needs to be a callable function, i used a lambda below in the test\n",
    "    #crit = torch.nn.MSELoss()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = get_batch(x, y, batch_size)\n",
    "    y_hat = model(x)\n",
    "    weights = w(x, y, model)\n",
    "    loss = torch.norm((y_hat - y))**2/batch_size\n",
    "    #loss = crit(y, y_hat)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "# gets parameters of model as array, used to check convergence later\n",
    "def get_params(model):\n",
    "    params=[]\n",
    "    param_generator = model.parameters()\n",
    "    for param in param_generator:\n",
    "        nums = param.flatten()\n",
    "        for num in nums:\n",
    "            params.append(num.item())\n",
    "    return np.array(params)\n",
    "\n",
    "# training loop that stops when parameters dont change much, we can explore other convergence criteria\n",
    "def train(x, y, model, optimizer, w, batch_size, eps):\n",
    "    done = False\n",
    "    params = get_params(model)\n",
    "    losses=[]\n",
    "    i=0\n",
    "    while not done:\n",
    "        i+=1\n",
    "        #print(i)\n",
    "        loss = step(x, y, model, optimizer, w, batch_size)\n",
    "        losses.append(loss)\n",
    "        new_params = get_params(model)\n",
    "        if np.linalg.norm(params-new_params)<eps:\n",
    "            done = True\n",
    "        else:\n",
    "            params = new_params\n",
    "    return losses   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate LP constraints\n",
    "\n",
    "d = 1 # number of equality constraints\n",
    "m = 2 # dimension of z\n",
    "np.random.seed(1)\n",
    "s0 = np.random.randn(d)\n",
    "s0 = np.maximum(s0, 0)\n",
    "rand_z = np.random.randn(m)\n",
    "A = np.random.randn(d, m) # equality constraint matrix\n",
    "b = A @ rand_z + s0 # vector for equality constraints\n",
    "\n",
    "\n",
    "epsilon = 1 # radius of wasserstein ball\n",
    "\n",
    "# maximum suboptimality weight calculation method\n",
    "# bad coding as I didn't pass epsilon, A or b as parameters but am using the global values from above...\n",
    "\n",
    "\n",
    "\n",
    "def ms(x, y, model):\n",
    "    sens = torch.zeros(len(x))\n",
    "    y_hat = model(x).detach().numpy()\n",
    "    for i in range(len(y_hat)):\n",
    "        \n",
    "        # solve decision problem to get z0\n",
    "        z = cp.Variable(m)\n",
    "        prob = cp.Problem(cp.Minimize(y_hat[i,:].T@z),\n",
    "                [A@z == b, z >= 0])\n",
    "        prob.solve()\n",
    "        z0 = z.value\n",
    "        #check if decision problem unbounded, infeasible, or has an optimal solution\n",
    "        print(\"decision problem\",prob.status)\n",
    "        best_value = -np.inf\n",
    "        for s in [-1,1]:\n",
    "            for j in range(y_hat.shape[1]):\n",
    "                z_s = cp.Variable(m)\n",
    "                h = cp.Variable(1)\n",
    "                prob = cp.Problem(cp.Maximize(y_hat[i,:].T@(z0 - z_s) + epsilon*h),\n",
    "                         [A@z == b, z_s >= 0, z0 - z_s <= h, z_s - z0 <= h, s*(z0[j] - z_s[j]) == h ])\n",
    "                prob.solve()\n",
    "                #check if sensitivity problem unbounded, infeasible, or has an optimal solution\n",
    "                print(\"sensitivity problem\",prob.status)\n",
    "                value = prob.value\n",
    "                print(\"decision z\", z0)\n",
    "                print(\"sensitivity z\", z_s.value)\n",
    "                print(\"sensitivity h\", h.value)\n",
    "                print(\"y_hat\", y_hat[i,:])\n",
    "                print(\"s\", s)\n",
    "                print(\"j\", j)\n",
    "                print(\"this_sens_val\", value)\n",
    "                \n",
    "                \n",
    "                \n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                \n",
    "        sens[i] = best_value\n",
    "        print(\"sensitivity\", sens[i])\n",
    "    return sens\n",
    "\n",
    "\n",
    "\n",
    "# generate data\n",
    "\n",
    "n=100 # number of obervations\n",
    "p = 3 # dimension of context variable x or number of predictors\n",
    "batch_size = 100\n",
    "eps = 1e-2 # convergence criteria\n",
    "sigma = .03\n",
    "\n",
    "\n",
    "true_model = torch.nn.Sequential(\n",
    "      torch.nn.Linear(in_features=p, out_features=m)\n",
    ")\n",
    "true_params = get_params(true_model)\n",
    "x = torch.randn(n, p)\n",
    "y = true_model(x) + sigma*torch.randn(n,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision problem optimal\n",
      "sensitivity problem unbounded\n",
      "decision z [-1.99947487e-09  2.10728086e+00]\n",
      "sensitivity z None\n",
      "sensitivity h None\n",
      "y_hat [-1.2737178  1.1735569]\n",
      "s -1\n",
      "j 0\n",
      "this_sens_val inf\n",
      "sensitivity problem unbounded\n",
      "decision z [-1.99947487e-09  2.10728086e+00]\n",
      "sensitivity z None\n",
      "sensitivity h None\n",
      "y_hat [-1.2737178  1.1735569]\n",
      "s -1\n",
      "j 1\n",
      "this_sens_val inf\n",
      "sensitivity problem optimal\n",
      "decision z [-1.99947487e-09  2.10728086e+00]\n",
      "sensitivity z [-1.77761010e-09  2.10728086e+00]\n",
      "sensitivity h [-1.89635184e-10]\n",
      "y_hat [-1.2737178  1.1735569]\n",
      "s 1\n",
      "j 0\n",
      "this_sens_val -4.255689134424756e-10\n",
      "sensitivity problem optimal\n",
      "decision z [-1.99947487e-09  2.10728086e+00]\n",
      "sensitivity z [ 2.10728086e+00 -2.92560756e-11]\n",
      "sensitivity h [2.10728086]\n",
      "y_hat [-1.2737178  1.1735569]\n",
      "s 1\n",
      "j 1\n",
      "this_sens_val 7.264375964260809\n",
      "sensitivity tensor(inf)\n",
      "decision problem optimal\n",
      "sensitivity problem unbounded\n",
      "decision z [-2.78722785e-09  2.10728086e+00]\n",
      "sensitivity z None\n",
      "sensitivity h None\n",
      "y_hat [0.19655249 0.27136976]\n",
      "s -1\n",
      "j 0\n",
      "this_sens_val inf\n",
      "sensitivity problem unbounded\n",
      "decision z [-2.78722785e-09  2.10728086e+00]\n",
      "sensitivity z None\n",
      "sensitivity h None\n",
      "y_hat [0.19655249 0.27136976]\n",
      "s -1\n",
      "j 1\n",
      "this_sens_val inf\n",
      "sensitivity problem optimal\n",
      "decision z [-2.78722785e-09  2.10728086e+00]\n",
      "sensitivity z [-1.94003000e-09  2.10728086e+00]\n",
      "sensitivity h [-4.56833017e-10]\n",
      "y_hat [0.19655249 0.27136976]\n",
      "s 1\n",
      "j 0\n",
      "this_sens_val -6.58169296841038e-10\n",
      "sensitivity problem optimal\n",
      "decision z [-2.78722785e-09  2.10728086e+00]\n",
      "sensitivity z [8.13674473e-09 4.50746461e-10]\n",
      "sensitivity h [2.10728086]\n",
      "y_hat [0.19655249 0.27136976]\n",
      "s 1\n",
      "j 1\n",
      "this_sens_val 2.6791331478721045\n",
      "sensitivity tensor(inf)\n",
      "decision problem unbounded\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e2a3d1c5067d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#yo = lambda x, c, batch_size: w(x, c, batch_size) # this is the callable function to calculate weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mend_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"true params are\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f88152cac43b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(x, y, model, optimizer, w, batch_size, eps)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m#print(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mnew_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f88152cac43b>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(x, y, model, optimizer, w, batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m#loss = crit(y, y_hat)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c4184632b468>\u001b[0m in \u001b[0;36mms\u001b[1;34m(x, y, model)\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 prob = cp.Problem(cp.Maximize(y_hat[i,:].T@(z0 - z_s) + epsilon*h),\n\u001b[1;32m---> 39\u001b[1;33m                          [A@z == b, z_s >= 0, z0 - z_s <= h, z_s - z0 <= h, s*(z0[j] - z_s[j]) == h ])\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[1;31m#check if sensitivity problem unbounded, infeasible, or has an optimal solution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# train sensitivity model\n",
    "model = torch.nn.Sequential(\n",
    "      torch.nn.Linear(in_features=p, out_features=m)\n",
    ")\n",
    "start_params = get_params(model)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "#yo = lambda x, c, batch_size: w(x, c, batch_size) # this is the callable function to calculate weights\n",
    "losses = train(x, y, model, optimizer, ms, batch_size, eps)\n",
    "end_params = get_params(model)\n",
    "print(\"true params are\", true_params)\n",
    "print(\"found params are\", end_params)\n",
    "print(\"relative errror is\", np.linalg.norm(end_params-true_params)/np.linalg.norm(true_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
